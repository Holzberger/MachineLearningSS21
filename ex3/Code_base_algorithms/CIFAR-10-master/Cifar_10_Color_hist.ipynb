{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import cv2\n",
    "#from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report , confusion_matrix , accuracy_score, f1_score, plot_confusion_matrix\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "def create_vectorizer(train_data, *kwargs, max_features=1):\n",
    "    vectorizer = CountVectorizer(max_features=max_features)\n",
    "    res        = [vectorizer]\n",
    "    res.append(vectorizer.fit_transform(train_data).toarray())\n",
    "    for arg in kwargs:\n",
    "        res.append(vectorizer.transform(arg).toarray())\n",
    "    return res\n",
    "\n",
    "scaler = None\n",
    "def create_scaling(X):\n",
    "    global scaler\n",
    "    scaler = StandardScaler().fit(X)\n",
    "\n",
    "def remove_outlyers(data, label, threshold=100):\n",
    "    mask = (data>threshold).sum(axis=1)==0\n",
    "    print(\"removing {} rows\\n\".format(np.sum(~mask)))\n",
    "    return [data[mask], label.drop(label.index[~mask])]\n",
    "\n",
    "def create_knn(train_data, target, **kwargs):\n",
    "    clf = KNeighborsClassifier(**kwargs)\n",
    "    clf.fit(train_data, target )\n",
    "    return clf\n",
    "\n",
    "def create_perceptron(train_data, target, grid=[], **kwargs):\n",
    "    if grid != []:\n",
    "        clf = GridSearchCV(Perceptron(), grid, refit = True, verbose = 3,n_jobs=2)\n",
    "    else:\n",
    "        clf = Perceptron(**kwargs)\n",
    "    clf.fit(train_data, target)\n",
    "    return clf\n",
    "\n",
    "def create_rnd_forrest(train_data, target, grid=[], **kwargs):\n",
    "    if grid != []:\n",
    "        clf = GridSearchCV(RandomForestClassifier(), grid, refit = True, verbose = 3,n_jobs=2)\n",
    "    else:\n",
    "        clf = RandomForestClassifier(**kwargs)\n",
    "    clf.fit(train_data, target)\n",
    "    return clf\n",
    "\n",
    "def create_nb(train_data, target, **kwargs):\n",
    "    clf =MultinomialNB( **kwargs)\n",
    "    clf.fit(train_data , target)\n",
    "    return clf\n",
    "    \n",
    "def _create_MLP(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation=\"relu\", input_shape=(input_dim,)))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_MLP(X_train, y_train):\n",
    "    \n",
    "    mlp = KerasClassifier(build_fn=(lambda: _create_MLP(X_train.shape[1])), epochs=15, batch_size=32, verbose=0)\n",
    "    mlp.fit(X_train, np_utils.to_categorical(y_train))\n",
    "    return mlp\n",
    "\n",
    "def plot_confusion_matrix_MLP(mlp, X_test, y_test):\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred, normalize='pred'))\n",
    "    disp.plot()\n",
    "    plt.title('Multi-layer perceptron confusion matrix')\n",
    "    plt.show()\n",
    "    \n",
    "def get_metrics(algo, test_data, test_target, train_data, train_target):\n",
    "    pred_test  = algo.predict(test_data)\n",
    "    pred_train = algo.predict(train_data)\n",
    "\n",
    "    return classification_report(test_target, pred_test,output_dict=True),\\\n",
    "           classification_report(train_target, pred_train,output_dict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = {}\n",
    "times_prepro = {}\n",
    "times_test = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Cifar using Tensor flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "(X_train,y_train) , (X_test,y_test) = cifar10.load_data()\n",
    "X_train = X_train[:1000]\n",
    "y_train = y_train[:1000]\n",
    "X_test = X_test[:250]\n",
    "y_test = y_test[:250]\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])\n",
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of image\n",
    "plt.imshow(X_train[10])\n",
    "plt.imshow(X_test[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of RGB histogram plot\n",
    "images=X_train[10]\n",
    "channels = [0] # [1] or [2] for green or red 0 is blue\n",
    "mask= None\n",
    "histSize = [256] # Number of bins, forr full scale take 256\n",
    "ranges= [0,256] # this is our RANGE. Normally, it is [0,256].\n",
    "hist_blue=cv2.calcHist(images, channels, mask, histSize, ranges)\n",
    "hist_green=cv2.calcHist(images, [1], mask, histSize, ranges)\n",
    "hist_red=cv2.calcHist(images, [2], mask, histSize, ranges)\n",
    "#plt.plot(hist_blue, hist_green, hist_red)\n",
    "plt.plot(hist_blue, color= 'blue')\n",
    "plt.plot(hist_green, color= 'green')\n",
    "plt.plot(hist_red , color ='red')\n",
    "print(hist_green.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_names[y_train[10][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask= None\n",
    "histSize = [256] # Number of bins, forr full scale take 256\n",
    "ranges= [0,256] # this is our RANGE. Normally, it is [0,256].\n",
    "\n",
    "times_prepro['histogram'] = time()\n",
    "data=[]\n",
    "for i in range(0,len(X_train)):\n",
    "    hist=np.empty((0,0))\n",
    "    images = X_train[i]\n",
    "    #imagePIL = imagePIL.convert('RGB')   \n",
    "    #images = imagePIL.convert('RGB') \n",
    "    hist_blue=cv2.calcHist(images, [0], mask, histSize, ranges)\n",
    "    hist_green=cv2.calcHist(images, [1], mask, histSize, ranges)\n",
    "    hist_red=cv2.calcHist(images, [2], mask, histSize, ranges)\n",
    "    hist = np.append(hist, hist_blue)\n",
    "    hist = np.append(hist, hist_green)\n",
    "    hist = np.append(hist, hist_red)\n",
    "    featureVector=hist\n",
    "    #if (len(featureVector) != 768): # just a sanity check; with the transformation to RGB, this should never happen\n",
    "    #    print(\"Unexpected length of feature vector: \" + i)\n",
    "    data.append((featureVector))\n",
    "np_data=np.array(data)\n",
    "print(np_data.shape)\n",
    "times_prepro['histogram'] = time() - times_prepro['histogram']\n",
    "\n",
    "# We have to convert the test set as well\n",
    "\n",
    "times_test['histogram'] = time()\n",
    "data=[]\n",
    "for i in range(0,len(X_test)):\n",
    "    hist=np.empty((0,0))\n",
    "    images = X_test[i]\n",
    "    #imagePIL = imagePIL.convert('RGB')   \n",
    "    #images = imagePIL.convert('RGB') \n",
    "    hist_blue=cv2.calcHist(images, [0], mask, histSize, ranges)\n",
    "    hist_green=cv2.calcHist(images, [1], mask, histSize, ranges)\n",
    "    hist_red=cv2.calcHist(images, [2], mask, histSize, ranges)\n",
    "    hist = np.append(hist, hist_blue)\n",
    "    hist = np.append(hist, hist_green)\n",
    "    hist = np.append(hist, hist_red)\n",
    "    featureVector=hist\n",
    "    #if (len(featureVector) != 768): # just a sanity check; with the transformation to RGB, this should never happen\n",
    "    #    print(\"Unexpected length of feature vector: \" + i)\n",
    "    data.append((featureVector))\n",
    "np_data_test=np.array(data)\n",
    "print(np_data_test.shape)\n",
    "times_test['histogram'] = time() - times_test['histogram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataOpenCV_1D=[]\n",
    "dataOpenCV_2D=[]\n",
    "dataOpenCV_3D=[]\n",
    "\n",
    "dataOpenCV_1D_test=[]\n",
    "dataOpenCV_2D_test=[]\n",
    "dataOpenCV_3D_test=[]\n",
    "\n",
    "times_prepro['histogram 1D'] = time()\n",
    "\n",
    "# use our own simple function to flatten the 2D arrays\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "for i in range(0,len(X_train)):\n",
    "    featuresOpenCV_1D = []\n",
    "    #hist=np.empty((0,0))\n",
    "    images = X_train[i]\n",
    "    \n",
    "    channels= ([0], [1], [2])\n",
    "    color = (\"b\", \"g\", \"r\")\n",
    "    \n",
    "    histSize_1d=[64] \n",
    "    featuresOpenCV_1D = []\n",
    "    for (channels, color) in zip(channels,color):\n",
    "        \n",
    "        # Not all images in our dataset are in RGB color scheme (e.g. indexed colours)\n",
    "        # We need to make sure that they are RGB , otherwise we can't expect to have exactly three RGB channels..\n",
    "        #imagePIL = imagePIL.convert('RGB')   \n",
    "        #images = imagePIL.convert('RGB') \n",
    "        hist_opencv=cv2.calcHist(images, channels, mask, histSize_1d, ranges)\n",
    "        featuresOpenCV_1D=np.append(featuresOpenCV_1D, hist_opencv)\n",
    "    featureVectorOpenCV_1D = featuresOpenCV_1D\n",
    "    dataOpenCV_1D.append(featureVectorOpenCV_1D) \n",
    "    \n",
    "    #if (len(featureVectorOpenCV_1D) != bins_1D*3): # sanity check, in case we had a wrong number of channels...\n",
    "    #    print \"Unexpected length of feature vector: \" + str(len(featureVectorOpenCV_1D)) + \" in file: \" + i\n",
    "    \n",
    "np_dataOpenCV_1D=np.array(dataOpenCV_1D)\n",
    "times_prepro['histogram 1D'] = time() - times_prepro['histogram 1D']\n",
    "\n",
    "\n",
    "times_test['histogram 1D'] = time()\n",
    "\n",
    "for i in range(0,len(X_test)):\n",
    "\n",
    "    #hist=np.empty((0,0))\n",
    "    images = X_test[i]\n",
    "    \n",
    "    channels= ([0], [1], [2])\n",
    "    color = (\"b\", \"g\", \"r\")\n",
    "    \n",
    "    histSize_1d=[64] \n",
    "    featuresOpenCV_1D = []\n",
    "    for (channels, color) in zip(channels,color):\n",
    "        #imagePIL = imagePIL.convert('RGB')   \n",
    "        #images = imagePIL.convert('RGB') \n",
    "        hist_opencv=cv2.calcHist(images, channels, mask, histSize_1d, ranges)\n",
    "        featuresOpenCV_1D=np.append(featuresOpenCV_1D, hist_opencv)\n",
    "    featureVectorOpenCV_1D = featuresOpenCV_1D\n",
    "    dataOpenCV_1D_test.append(featureVectorOpenCV_1D) \n",
    "    \n",
    "    #if (len(featureVectorOpenCV_1D) != bins_1D*3): # sanity check, in case we had a wrong number of channels...\n",
    "    #    print \"Unexpected length of feature vector: \" + str(len(featureVectorOpenCV_1D)) + \" in file: \" + i\n",
    "    \n",
    "np_dataOpenCV_1D_test=np.array(dataOpenCV_1D_test)\n",
    "times_test['histogram 1D'] = time() - times_test['histogram 1D']\n",
    "\n",
    "#print(data)\n",
    "print(np_dataOpenCV_1D.shape)\n",
    "print(np_dataOpenCV_1D)\n",
    "print(np_dataOpenCV_1D_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataOpenCV_2D=[]\n",
    "dataOpenCV_2D_test=[]\n",
    "\n",
    "times_prepro['histogram 2D'] = time()\n",
    "\n",
    "for i in range(0,len(X_train)):\n",
    "    featuresOpenCV_2D = []\n",
    "    #hist=np.empty((0,0))\n",
    "    images = X_train[i]\n",
    "    \n",
    "    channels= [[0], [1], [2]]\n",
    "    color = (\"b\", \"g\", \"r\")\n",
    "    \n",
    "    histSize_2d= 16 \n",
    "    featuresOpenCV_2D = []\n",
    "        \n",
    "        # Not all images in our dataset are in RGB color scheme (e.g. indexed colours)\n",
    "        # We need to make sure that they are RGB , otherwise we can't expect to have exactly three RGB channels..\n",
    "        #imagePIL = imagePIL.convert('RGB')   \n",
    "        #images = imagePIL.convert('RGB') \n",
    "    hist_opencv_2d_1 = cv2.calcHist(images,  [0,1], mask, [histSize_2d, histSize_2d], [0, 256, 0, 256])\n",
    "    hist_opencv_2d_2 = cv2.calcHist(images,  [0,2], mask, [histSize_2d, histSize_2d], [0, 256, 0, 256])\n",
    "    hist_opencv_2d_3 = cv2.calcHist(images,  [1,2], mask, [histSize_2d, histSize_2d], [0, 256, 0, 256])\n",
    "    featuresOpenCV_2D= np.append(featuresOpenCV_2D, hist_opencv_2d_1)\n",
    "    featuresOpenCV_2D= np.append(featuresOpenCV_2D, hist_opencv_2d_2)\n",
    "    featuresOpenCV_2D= np.append(featuresOpenCV_2D, hist_opencv_2d_3)\n",
    "    featureVectorOpenCV_2D = featuresOpenCV_2D # and append this to our feature vector\n",
    "    dataOpenCV_2D.append(featureVectorOpenCV_2D) # now we append the feature vector to the dataset so far\n",
    "    \n",
    "    #if (len(featureVectorOpenCV_1D) != bins_1D*3): # sanity check, in case we had a wrong number of channels...\n",
    "    #    print \"Unexpected length of feature vector: \" + str(len(featureVectorOpenCV_1D)) + \" in file: \" + i\n",
    "    \n",
    "np_dataOpenCV_2D= np.array(dataOpenCV_2D)\n",
    "times_prepro['histogram 2D'] = time() - times_prepro['histogram 2D']\n",
    "\n",
    "# We have to extract features for the test set as well\n",
    "\n",
    "times_test['histogram 2D'] = time()\n",
    "\n",
    "for i in range(0,len(X_test)):\n",
    "    featuresOpenCV_2D = []\n",
    "    #hist=np.empty((0,0))\n",
    "    images = X_test[i]\n",
    "    \n",
    "    channels= [[0], [1], [2]]\n",
    "    color = (\"b\", \"g\", \"r\")\n",
    "    \n",
    "    histSize_2d= 16 \n",
    "    featuresOpenCV_2D = []\n",
    "        \n",
    "        # Not all images in our dataset are in RGB color scheme (e.g. indexed colours)\n",
    "        # We need to make sure that they are RGB , otherwise we can't expect to have exactly three RGB channels..\n",
    "        #imagePIL = imagePIL.convert('RGB')   \n",
    "        #images = imagePIL.convert('RGB') \n",
    "    hist_opencv_2d_1 = cv2.calcHist(images,  [0,1], mask, [histSize_2d, histSize_2d], [0, 256, 0, 256])\n",
    "    hist_opencv_2d_2 = cv2.calcHist(images,  [0,2], mask, [histSize_2d, histSize_2d], [0, 256, 0, 256])\n",
    "    hist_opencv_2d_3 = cv2.calcHist(images,  [1,2], mask, [histSize_2d, histSize_2d], [0, 256, 0, 256])\n",
    "    featuresOpenCV_2D= np.append(featuresOpenCV_2D, hist_opencv_2d_1)\n",
    "    featuresOpenCV_2D= np.append(featuresOpenCV_2D, hist_opencv_2d_2)\n",
    "    featuresOpenCV_2D= np.append(featuresOpenCV_2D, hist_opencv_2d_3)\n",
    "    featureVectorOpenCV_2D = featuresOpenCV_2D # and append this to our feature vector\n",
    "    dataOpenCV_2D_test.append(featureVectorOpenCV_2D) # now we append the feature vector to the dataset so far\n",
    "    \n",
    "    #if (len(featureVectorOpenCV_1D) != bins_1D*3): # sanity check, in case we had a wrong number of channels...\n",
    "    #    print \"Unexpected length of feature vector: \" + str(len(featureVectorOpenCV_1D)) + \" in file: \" + i\n",
    "    \n",
    "np_dataOpenCV_2D_test= np.array(dataOpenCV_2D_test)\n",
    "times_test['histogram 2D'] = time() - times_test['histogram 2D']\n",
    "\n",
    "#print(data)\n",
    "print(np_dataOpenCV_2D.shape)\n",
    "print(np_dataOpenCV_2D_test.shape)\n",
    "print(np_dataOpenCV_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataOpenCV_3D=[]\n",
    "dataOpenCV_3D_test=[]\n",
    "\n",
    "times_prepro['histogram 3D'] = time()\n",
    "\n",
    "for i in range(0,len(X_train)):\n",
    "    featuresOpenCV_2D = []\n",
    "    #hist=np.empty((0,0))\n",
    "    images = X_train[i]\n",
    "    \n",
    "    channels= [[0], [1], [2]]\n",
    "    color = (\"b\", \"g\", \"r\")\n",
    "    \n",
    "    histSize_3d= 8 \n",
    "    featuresOpenCV_3D = []\n",
    "\n",
    "    #imagePIL = imagePIL.convert('RGB')   \n",
    "    #images = imagePIL.convert('RGB') \n",
    "    hist_opencv_3d = cv2.calcHist(images,  [0,1,2], mask, [histSize_3d, histSize_3d,histSize_3d ], [0, 256, 0, 256,  0, 256])\n",
    "    featuresOpenCV_3D= np.append(featuresOpenCV_2D, hist_opencv_3d)\n",
    "    featureVectorOpenCV_3D = featuresOpenCV_3D # and append this to our feature vector\n",
    "    dataOpenCV_3D.append(featureVectorOpenCV_3D) # now we append the feature vector to the dataset so far\n",
    " \n",
    "np_dataOpenCV_3D = np.array(dataOpenCV_3D)\n",
    "times_prepro['histogram 3D'] = time() - times_prepro['histogram 3D']\n",
    "\n",
    "# and for the test set\n",
    "\n",
    "times_test['histogram 3D'] = time()\n",
    "\n",
    "for i in range(0,len(X_test)):\n",
    "    featuresOpenCV_2D = []\n",
    "    #hist=np.empty((0,0))\n",
    "    images = X_test[i]\n",
    "    \n",
    "    channels= [[0], [1], [2]]\n",
    "    color = (\"b\", \"g\", \"r\")\n",
    "    \n",
    "    histSize_3d= 8 \n",
    "    featuresOpenCV_3D = []\n",
    "\n",
    "    #imagePIL = imagePIL.convert('RGB')   \n",
    "    #images = imagePIL.convert('RGB') \n",
    "    hist_opencv_3d = cv2.calcHist(images,  [0,1,2], mask, [histSize_3d, histSize_3d,histSize_3d ], [0, 256, 0, 256,  0, 256])\n",
    "    featuresOpenCV_3D= np.append(featuresOpenCV_2D, hist_opencv_3d)\n",
    "    featureVectorOpenCV_3D = featuresOpenCV_3D # and append this to our feature vector\n",
    "    dataOpenCV_3D_test.append(featureVectorOpenCV_3D) # now we append the feature vector to the dataset so far\n",
    "\n",
    "np_dataOpenCV_3D_test= np.array(dataOpenCV_3D_test)\n",
    "times_test['histogram 3D'] = time() - times_test['histogram 3D']\n",
    "\n",
    "print(np_dataOpenCV_3D.shape)\n",
    "print(np_dataOpenCV_3D)\n",
    "print(np_dataOpenCV_3D_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score_MLP(mlp, X, Y, cv, epochs, batch_size):\n",
    "    kfold = KFold(n_splits=cv, shuffle=True)\n",
    "    acc_per_fold = []\n",
    "    for train, test in kfold.split(X, Y):\n",
    "        train_data = np.take(X, train, 0)\n",
    "        train_target = np.take(Y, train, 0)\n",
    "        test_data = np.take(X, test, 0)\n",
    "        test_target = np.take(Y, test, 0)\n",
    "        history = mlp.fit(train_data, np_utils.to_categorical(train_target), batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "        y_pred = mlp.predict(test_data)\n",
    "        #print(y_pred)\n",
    "        score = accuracy_score(y_pred, test_target)\n",
    "        acc_per_fold.append(score)\n",
    "    return np.array(acc_per_fold)\n",
    "\n",
    "# Classification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# these are our feature sets; we will use each of them individually to train classifiers\n",
    "trainingSets = [np_data, np_dataOpenCV_1D, np_dataOpenCV_2D, np_dataOpenCV_3D]\n",
    "vals      = trainingSets\n",
    "TestSets = [np_data_test, np_dataOpenCV_1D_test, np_dataOpenCV_2D_test, np_dataOpenCV_3D_test]\n",
    "vals_test      = TestSets\n",
    "trainingLabel=y_train\n",
    "scores_ppn, scores_NB, scores_rndf, scores_mlp = [], [], [], []\n",
    "scores_ppn_test, scores_NB_test, scores_rndf_test, scores_mlp_test = [], [], [], []\n",
    "\n",
    "for X, X_t in zip(vals, vals_test):\n",
    "    \n",
    "    ppn = create_perceptron(pd.DataFrame(X), pd.DataFrame(trainingLabel))\n",
    "    scores_ppn.append(cross_val_score(ppn, pd.DataFrame(X), pd.DataFrame(trainingLabel), scoring=\"accuracy\", cv=10).mean())\n",
    "    scores_ppn_test.append(accuracy_score(y_test, ppn.predict(X_t),normalize=True))\n",
    "\n",
    "    \n",
    "    NB = create_nb(pd.DataFrame(X), pd.DataFrame(trainingLabel))\n",
    "    scores_NB.append(cross_val_score(NB, pd.DataFrame(X), pd.DataFrame(trainingLabel), scoring=\"accuracy\", cv=10).mean())\n",
    "    scores_NB_test.append(accuracy_score(y_test, NB.predict(X_t),normalize=True))\n",
    "    \n",
    "    rndf = create_rnd_forrest(pd.DataFrame(X), pd.DataFrame(trainingLabel),random_state=42, n_estimators=50)\n",
    "    scores_rndf.append(cross_val_score(rndf, pd.DataFrame(X), pd.DataFrame(trainingLabel), scoring=\"accuracy\", cv=10).mean())\n",
    "    scores_rndf_test.append(accuracy_score(y_test, rndf.predict(X_t),normalize=True))\n",
    "    \n",
    "    create_scaling(X)\n",
    "    X = scaler.transform(X)\n",
    "    \n",
    "    mlp = create_MLP(X, trainingLabel)\n",
    "    scores_mlp.append(calc_score_MLP(mlp, X, trainingLabel, cv=10, epochs=15, batch_size=32).mean())\n",
    "    scores_mlp_test.append(accuracy_score(y_test, mlp.predict(X_t),normalize=True))\n",
    "        \n",
    "print(scores_ppn)\n",
    "print(scores_NB)\n",
    "print(scores_rndf)\n",
    "print(scores_mlp)\n",
    "print(scores_ppn_test)\n",
    "\n",
    "Nr_data = np.arange(4)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(Nr_data, scores_ppn,\"--\",color=\"orange\",marker=\"*\", linewidth=1,label=\"ppn\")\n",
    "plt.plot(Nr_data, scores_NB,\"--\",color=\"blue\",marker=\"*\", linewidth=1,label=\"NB\")\n",
    "plt.plot(Nr_data, scores_rndf,\"--\",color=\"black\",marker=\"*\", linewidth=1,label=\"rndf\")\n",
    "plt.plot(Nr_data, scores_mlp,\"--\",color=\"green\",marker=\"*\", linewidth=1,label=\"mlp\")\n",
    "plt.plot(Nr_data, scores_ppn_test,\"-\",color=\"orange\",marker=\"*\", linewidth=1,label=\"ppn\")\n",
    "plt.plot(Nr_data, scores_NB_test,\"-\",color=\"blue\",marker=\"*\", linewidth=1,label=\"NB\")\n",
    "plt.plot(Nr_data, scores_rndf_test,\"-\",color=\"black\",marker=\"*\", linewidth=1,label=\"rndf\")\n",
    "plt.plot(Nr_data, scores_mlp_test,\"-\",color=\"green\",marker=\"*\", linewidth=1,label=\"mlp\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Color Histogram\",fontsize=12)\n",
    "plt.ylabel(\"accuracy\",fontsize=12)\n",
    "plt.xlabel(\"Feature extractions\",fontsize=12)\n",
    "labels =[\"1D RBG 256\", \"1D RBG 64\", \"2D RBG\", \"3D RBG\"]\n",
    "plt.xticks(Nr_data, labels,rotation=30,fontsize=11, ha=\"right\")\n",
    "plt.savefig(\"./Accuracy_color_hist.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(np_dataOpenCV_3D, trainingLabel, test_size=0.2, random_state=42)\n",
    "X_train = np_dataOpenCV_2D\n",
    "X_test = np_dataOpenCV_2D_test\n",
    "\n",
    "times['histo ppn'] = time()\n",
    "ppn = create_perceptron(X_train, y_train)\n",
    "times['histo ppn'] = time() - times['histo ppn']\n",
    "plot_confusion_matrix(ppn, X_test, y_test, normalize='pred')\n",
    "plt.title('Perceptron confusion matrix')\n",
    "plt.savefig(\"./CM__ppn_color_hist.pdf\")\n",
    "\n",
    "times['histo nb'] = time()\n",
    "NB = create_nb(X_train, y_train)\n",
    "times['histo nb'] = time() - times['histo nb']\n",
    "plot_confusion_matrix(NB, X_test, y_test, normalize='pred')\n",
    "plt.title('Naive Bayes confusion matrix')\n",
    "plt.savefig(\"./CM__NB_color_hist.pdf\")\n",
    "\n",
    "times['histo rndf'] = time()\n",
    "rndf = create_rnd_forrest(X_train, y_train, random_state=42, n_estimators=50)\n",
    "times['histo rndf'] = time() - times['histo rndf']\n",
    "plot_confusion_matrix(rndf, X_test, y_test, normalize='pred')\n",
    "plt.title('Random forest confusion matrix')\n",
    "plt.savefig(\"./CM__Rndf_color_hist.pdf\")\n",
    "\n",
    "times_prepro['scaling'] = time()\n",
    "create_scaling(X_train)\n",
    "X_train, X_test = scaler.transform(X_train), scaler.transform(X_test)\n",
    "times_prepro['scaling'] = time() - times_prepro['scaling']\n",
    "\n",
    "times['histo mlp'] = time()\n",
    "mlp = create_MLP(X_train, y_train)\n",
    "times['histo mlp'] = time() - times['histo mlp']\n",
    "plot_confusion_matrix_MLP(mlp, X_test, y_test)\n",
    "plt.savefig(\"./CM__MLP_color_hist.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train) , (X_test,y_test) = cifar10.load_data()\n",
    "Image_grey=cv2.cvtColor(X_train[0], cv2.COLOR_BGR2GRAY)\n",
    "X_train = X_train[:1000]\n",
    "y_train = y_train[:1000]\n",
    "X_test = X_test[:250]\n",
    "y_test = y_test[:250]\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Image_grey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT\n",
    "sift=cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# example of descriptors\n",
    "keypoints, descriptors = sift.detectAndCompute(X_train[0], None)\n",
    "keypoints, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize X_train\n",
    "X, Y = [], []\n",
    "\n",
    "times_prepro['sift+bow'] = time()\n",
    "# extract descriptors\n",
    "descriptor_list = []\n",
    "for i in range(len(X_train)):\n",
    "    keypoints, descriptors = sift.detectAndCompute(X_train[i], None)\n",
    "    if descriptors is None:\n",
    "        plt.imshow(X_train[i])\n",
    "    else:\n",
    "        descriptor_list.append(descriptors)\n",
    "        X.append(X_train[i])\n",
    "        Y.append(y_train[i])\n",
    "\n",
    "X_train = np.array(X)\n",
    "y_train = np.array(Y)\n",
    "input_size = len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor_vstack = np.array(descriptor_list[0])\n",
    "for remaining in descriptor_list[1:]:\n",
    "    descriptor_vstack = np.vstack((descriptor_vstack, remaining))\n",
    "descriptor_vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create clusters\n",
    "n_clusters = 20\n",
    "kmeans = KMeans(n_clusters = n_clusters)\n",
    "kmeans = kmeans.fit_predict(descriptor_vstack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mega histogram\n",
    "mega_histogram = np.array([np.zeros(n_clusters) for i in range(input_size)])\n",
    "old_count = 0\n",
    "for i in range(input_size): \n",
    "    l = len(descriptor_list[i])\n",
    "    for j in range(l):\n",
    "        idx = kmeans[old_count+j]\n",
    "        mega_histogram[i][idx] += 1\n",
    "    old_count += l\n",
    "print(\"Vocabulary Histogram Generated\")\n",
    "mega_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize\n",
    "scale = StandardScaler().fit(mega_histogram)\n",
    "X = scale.transform(mega_histogram)\n",
    "times_prepro['sift+bow'] = time() - times_prepro['sift+bow']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "\n",
    "trainingLabel=y_train[:input_size]\n",
    "\n",
    "times['bow ppn'] = time()\n",
    "ppn = create_perceptron(pd.DataFrame(X), pd.DataFrame(trainingLabel))\n",
    "times['bow ppn'] = time() - times['bow ppn']\n",
    "score_ppn = cross_val_score(ppn, pd.DataFrame(X), pd.DataFrame(trainingLabel), scoring=\"accuracy\", cv=10).mean()\n",
    "\n",
    "times['bow nb'] = time()\n",
    "NB = create_nb(pd.DataFrame(mega_histogram), pd.DataFrame(trainingLabel)) # cannot take negative values\n",
    "times['bow nb'] = time() - times['bow nb']\n",
    "score_NB = cross_val_score(NB, pd.DataFrame(mega_histogram), pd.DataFrame(trainingLabel), scoring=\"accuracy\", cv=10).mean()\n",
    "\n",
    "times['bow rndf'] = time()\n",
    "rndf = create_rnd_forrest(pd.DataFrame(X), pd.DataFrame(trainingLabel),random_state=42, n_estimators=50)\n",
    "times['bow rndf'] = time() - times['bow rndf']\n",
    "score_rndf = cross_val_score(rndf, pd.DataFrame(X), pd.DataFrame(trainingLabel), scoring=\"accuracy\", cv=10).mean()\n",
    "\n",
    "times['bow mlp'] = time()\n",
    "mlp = create_MLP(X, trainingLabel)\n",
    "times['bow mlp'] = time() - times['bow mlp']\n",
    "score_mlp = calc_score_MLP(mlp, X, trainingLabel, cv=10, epochs=15, batch_size=32).mean()\n",
    "    \n",
    "print(score_ppn)\n",
    "print(score_NB)\n",
    "print(score_rndf)\n",
    "print(score_mlp)\n",
    "\n",
    "\n",
    "objects = ('Perceptron', 'Naive Bayes', 'Random Forest', 'MLP')\n",
    "y_pos = np.arange(len(objects))\n",
    "scores = [score_ppn, score_NB, score_rndf, score_mlp]\n",
    "\n",
    "plt.bar(y_pos, scores, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy with bag of words')\n",
    "\n",
    "plt.savefig(\"./Accuracy_BOW.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X_train, _X_test, y_train, y_test = train_test_split(mega_histogram, trainingLabel, test_size=0.2, random_state=42)\n",
    "X_train, X_test = scale.transform(_X_train), scale.transform(_X_test)\n",
    "\n",
    "ppn = create_perceptron(X_train, y_train)\n",
    "plot_confusion_matrix(ppn, X_test, y_test, normalize='pred')\n",
    "plt.title('Perceptron confusion matrix')\n",
    "plt.savefig(\"./CM__ppn_BOW.pdf\")\n",
    "\n",
    "NB = create_nb(_X_train, y_train)\n",
    "plot_confusion_matrix(NB, _X_test, y_test, normalize='pred')\n",
    "plt.title('Naive Bayes confusion matrix')\n",
    "plt.savefig(\"./CM__NB_BOW.pdf\")\n",
    "\n",
    "rndf = create_rnd_forrest(X_train, y_train, random_state=42, n_estimators=50)\n",
    "plot_confusion_matrix(rndf, X_test, y_test, normalize='pred')\n",
    "plt.title('Random forest confusion matrix')\n",
    "plt.savefig(\"./CM__Rndf_BOW.pdf\")\n",
    "\n",
    "mlp = create_MLP(X_train, y_train)\n",
    "plot_confusion_matrix_MLP(mlp, X_test, y_test)\n",
    "plt.savefig(\"./CM__MLP_BOW.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = times_prepro.keys()\n",
    "y_pos = np.arange(len(objects))\n",
    "scores = times_prepro.values()\n",
    "\n",
    "plt.bar(y_pos, scores, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects, rotation=45)\n",
    "plt.ylabel('Time')\n",
    "plt.title('Time with several methods of preprocessing')\n",
    "\n",
    "plt.savefig(\"./time_prepro.pdf\")\n",
    "plt.show()\n",
    "\n",
    "objects = times.keys()\n",
    "y_pos = np.arange(len(objects))\n",
    "scores = times.values()\n",
    "\n",
    "plt.bar(y_pos, scores, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects, rotation=45)\n",
    "plt.ylabel('Time')\n",
    "plt.title('Time with several methods of training')\n",
    "\n",
    "plt.savefig(\"./time_training.pdf\")\n",
    "plt.show()\n",
    "\n",
    "objects = times_test.keys()\n",
    "y_pos = np.arange(len(objects))\n",
    "scores = times_test.values()\n",
    "\n",
    "plt.bar(y_pos, scores, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects, rotation=45)\n",
    "plt.ylabel('Time')\n",
    "plt.title('Time with several methods of preprocessing for test')\n",
    "\n",
    "plt.savefig(\"./time_training_test.pdf\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
