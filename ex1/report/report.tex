\documentclass[11pt]{article}
\input{Packages}


\title{Exercise 1 Classification}
\author{e12045110 Maria de Ronde \\ e12045110  Quentin Andre  \\ e11921655 Fabian Holzberger}
\date{\today}

\begin{document}
\graphicspath{{./figures/}}
\maketitle

%
\section{Introduction}

\subsection{Applied Algorithms}
\subsection{Performance Metrics}

\section{Amazon Reviews Dataset}
\subsection{Dataset Description}
\subsection{Pre-Processing}
\subsection{Parameter-Tuning}
\subsection{Performance-Analysis}

\section{Congressional Voting Dataset}
\subsection{Dataset Description}
\subsection{Pre-Processing}
\subsection{Parameter-Tuning}
\subsection{Performance-Analysis}


\section{Email Spam Dataset(\href{https://www.kaggle.com/nitishabharathi/email-spam-dataset}{link to dataset})}


\subsection{Dataset Description}
The task of the email spam dataset is to predict if an email is spam or not. The link above contains three datasets from which we choose two, namely the \texttt{lingSpam.csv} and \texttt{enormSpamSubset.csv} for our project, since they have no missing values and the same layout. The dataset contains 12604 emails where 43.11% are spam and 56.89% are non-spam emails.The basic structure of emails can be seen in figure \ref{tab_spam0}.

\begin{figure}[h]
  \begin{tabular}{ | c | p{15cm} | c |}
    \hline
    Index & Body & Label \\
    \hline
    100 & 
    Subject: inexpensive online medication here
 pummel wah springtail cutler bodyguard
 we ship quality medications overnight to your door !...
    & 1 \\ \hline
    6006
    &
    Subject: organizational changes
 we are pleased to announce the following organizational changes :
 enron global assets and services
 in order to increase senior management focus on our international businesses... 
    & 0 \\
    \hline
    \end{tabular}
    \caption{Structure of the Email-Spam Dataset}
    \label{tab_spam0}
  \end{figure}
Every email has a binary target-label assigned, such that a 0 marks non-spam and a 1 marks spam emails. In figure \ref{fig_fig0} the distribution of the characters per email is shown. We see that most emails have a lenght in the range of 100 to 10.000 characters.

\begin{figure}
\begin{minipage}[t]{0.3\textwidth}
\includegraphics[width=1\linewidth]{email_spam/char_count.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\includegraphics[width=1\linewidth]{email_spam/word_count.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\includegraphics[width=1\linewidth]{email_spam/word_transformed_count.pdf}
\end{minipage}
   \caption{left: Distribution of e-mail lenghts, middle: Distribution of maximum norm in extracted word vectors with lenght 8000, right: Distribution of maximum norm in extracted word vectors with lenght 8000 after removing outlyers and applying logarithmic transformation}
\label{fig_fig0}
\end{figure}

\subsection{Pre-Processing}
In the dataset are 213 duplicate emails that we first remove and then peform the train/test-split where the trainset size is $20\%$ of the original dataset. Next we apply the Bag of Words feature extractor to each email. The algorithm converts every email to a vector $v\in\mathbb{Z}^N$ of intergers. First we create a list of all words and count their occurences in all emails. Then we take the $N$ most common words and count the occurences of the most common words in every email to get $v$. Before applying the Bag of word extractor we pre-process emails by the following steps:1. remove links (http...), 2. remove all characters exept alphabetical chars and numbers, 3. convert uppercase to lowercase, 4. split text-bodys into separate words, 5. lemmatize all words, 6. remove stopwords. For the steps 5., 6. we use nltk python package. By the preprocessing we reduce the number of distinct words from 126019 to 103759 words. 

In figure \ref{fig_fig0} middle we see the distribution of the maximum norm of the extracted vectors. One can identify that the maximum norm spans several orders of magnitude from $0$ to $10^5$. Especially there is only one vector $v$ with $||v||_\infty>10^4$. Outlyers with $||v||_\infty>10^3$ are therefor removed in the testset. Additionally we apply the logarithmic transformation $\log_{10}(x+1)$ to all the elements of a vector and obtain a $||\cdot||_\infty$ distrubution that is bounded by the maximum magnithude. Note that we add $1$ to all components of a vector since this component is $0$ after the logarithmic transformation. The distribution after the transformation is shown in figure \ref{fig_fig0} right.




\subsection{Parameter-Tuning}

\begin{figure}
\begin{minipage}[l]{0.3\textwidth}
\includegraphics[width=1\linewidth]{email_spam/perc_scaling.pdf}
\end{minipage}
\begin{minipage}[l]{0.3\textwidth}
\includegraphics[width=1\linewidth]{email_spam/perc_features.pdf}
\end{minipage}
\begin{minipage}[l]{0.3\textwidth}
\includegraphics[width=1\linewidth]{email_spam/perc_learning_rate.pdf}
\end{minipage}\\
\begin{minipage}[t]{0.3\textwidth}
\includegraphics[width=1\linewidth]{email_spam/perc_tolerance.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\includegraphics[width=1\linewidth]{email_spam/perc_maxiter.pdf}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
\includegraphics[width=1\linewidth]{email_spam/word_transformed_count.pdf}
\end{minipage}
   \caption{left: Distribution of e-mail lenghts, middle: Distribution of maximum norm in extracted word vectors with lenght 8000, right: Distribution of maximum norm in extracted word vectors with lenght 8000 after removing outlyers and applying logarithmic transformation}
\label{fig_fig1}
\end{figure}

\subsection{Performance-Analysis}


\section{Bridges Dataset(\href{https://archive.ics.uci.edu/ml/datasets/Pittsburgh+Bridges}{link to dataset})}
\subsection{Dataset Description}
\subsection{Pre-Processing}
\subsection{Parameter-Tuning}
\subsection{Performance-Analysis}

\section{Conclusion}

%Bibliography
\newpage
\bibliographystyle{plain}
\bibliography{Biblothek}

\end{document}
